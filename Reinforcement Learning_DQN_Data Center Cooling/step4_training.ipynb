{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBEDRmtUnllY",
    "outputId": "3fd32330-9de2-4a53-e2c6-d2e50225d79b"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')\n",
    "#%cd /gdrive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9cB0mq-njCL",
    "outputId": "11d70895-418b-43e3-89eb-761e54373813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 001/100\n",
      "Total Energy spent with an AI: 69\n",
      "Total Energy spent with no AI: 168\n",
      "\n",
      "\n",
      "Epoch: 002/100\n",
      "Total Energy spent with an AI: 104\n",
      "Total Energy spent with no AI: 264\n",
      "\n",
      "\n",
      "Epoch: 003/100\n",
      "Total Energy spent with an AI: 9\n",
      "Total Energy spent with no AI: 27\n",
      "\n",
      "\n",
      "Epoch: 004/100\n",
      "Total Energy spent with an AI: 98\n",
      "Total Energy spent with no AI: 370\n",
      "\n",
      "\n",
      "Epoch: 005/100\n",
      "Total Energy spent with an AI: 120\n",
      "Total Energy spent with no AI: 339\n",
      "\n",
      "\n",
      "Epoch: 006/100\n",
      "Total Energy spent with an AI: 4\n",
      "Total Energy spent with no AI: 18\n",
      "\n",
      "\n",
      "Epoch: 007/100\n",
      "Total Energy spent with an AI: 94\n",
      "Total Energy spent with no AI: 210\n",
      "\n",
      "\n",
      "Epoch: 008/100\n",
      "Total Energy spent with an AI: 6\n",
      "Total Energy spent with no AI: 11\n",
      "\n",
      "\n",
      "Epoch: 009/100\n",
      "Total Energy spent with an AI: 214\n",
      "Total Energy spent with no AI: 690\n",
      "\n",
      "\n",
      "Epoch: 010/100\n",
      "Total Energy spent with an AI: 12\n",
      "Total Energy spent with no AI: 22\n",
      "\n",
      "\n",
      "Epoch: 011/100\n",
      "Total Energy spent with an AI: 162\n",
      "Total Energy spent with no AI: 460\n",
      "\n",
      "\n",
      "Epoch: 012/100\n",
      "Total Energy spent with an AI: 172\n",
      "Total Energy spent with no AI: 621\n",
      "\n",
      "\n",
      "Epoch: 013/100\n",
      "Total Energy spent with an AI: 172\n",
      "Total Energy spent with no AI: 508\n",
      "\n",
      "\n",
      "Epoch: 014/100\n",
      "Total Energy spent with an AI: 69\n",
      "Total Energy spent with no AI: 178\n",
      "\n",
      "\n",
      "Epoch: 015/100\n",
      "Total Energy spent with an AI: 128\n",
      "Total Energy spent with no AI: 312\n",
      "\n",
      "\n",
      "Epoch: 016/100\n",
      "Total Energy spent with an AI: 72\n",
      "Total Energy spent with no AI: 216\n",
      "\n",
      "\n",
      "Epoch: 017/100\n",
      "Total Energy spent with an AI: 16\n",
      "Total Energy spent with no AI: 80\n",
      "\n",
      "\n",
      "Epoch: 018/100\n",
      "Total Energy spent with an AI: 128\n",
      "Total Energy spent with no AI: 282\n",
      "\n",
      "\n",
      "Epoch: 019/100\n",
      "Total Energy spent with an AI: 98\n",
      "Total Energy spent with no AI: 340\n",
      "Early Stopping\n"
     ]
    }
   ],
   "source": [
    "# Training the AI\n",
    "# Installing Keras\n",
    "# conda install -c conda-forge keras\n",
    "\n",
    "# Importing the libraries and the other python files\n",
    "import os\n",
    "import numpy as np\n",
    "import random as rn\n",
    "\n",
    "import step1_env as environment\n",
    "import step2_buildingbrain as brain\n",
    "import step3_dqn_rl_algo as dqn\n",
    "\n",
    "# Setting seeds for reproducibility \n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "rn.seed(12345)\n",
    "\n",
    "\n",
    "# SETTING THE PARAMETERS\n",
    "run = 'model' ## name for the run. Used to save the model file.\n",
    "epsilon = .3\n",
    "number_actions = 5\n",
    "direction_boundary = (number_actions - 1) / 2\n",
    "number_epochs = 100\n",
    "max_memory = 3000 \n",
    "batch_size = 512 \n",
    "temperature_step = 1.5\n",
    "\n",
    "\n",
    "# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
    "env = environment.Environment(optimal_temperature = (18.0, 24.0),\n",
    "                              initial_month = 0,\n",
    "                              initial_number_users = 20,\n",
    "                              initial_rate_data = 30)\n",
    "\n",
    "\n",
    "# BUILDING THE BRAIN BY SIMPLY CREATING AN OBJECT OF THE BRAIN CLASS\n",
    "brain = brain.Brain(learning_rate = 0.00001, number_actions = number_actions)\n",
    "\n",
    "\n",
    "# BUILDING THE DQN MODEL BY SIMPLY CREATING AN OBJECT OF THE DQN CLASS\n",
    "dqn = dqn.DQN(max_memory = max_memory, discount = 0.9)\n",
    "\n",
    "\n",
    "# CHOOSING THE MODE\n",
    "train = True\n",
    "\n",
    "# TRAINING THE AI\n",
    "env.train = train\n",
    "model = brain.model\n",
    "early_stopping = True\n",
    "patience = 10\n",
    "best_total_reward = -np.inf\n",
    "patience_count = 0\n",
    "\n",
    "\n",
    "if (env.train):\n",
    "\t# STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)\n",
    "\tfor epoch in range(1, number_epochs):\n",
    "\t\t# INITIALIAZING ALL THE VARIABLES OF BOTH THE ENVIRONMENT AND THE TRAINING LOOP\n",
    "\t\ttotal_reward = 0\n",
    "\t\tloss = 0.\n",
    "\t\tnew_month = np.random.randint(0, 12)\n",
    "\t\tenv.reset(new_month = new_month)\n",
    "\t\tgame_over = False\n",
    "\t\tcurrent_state, _, _ = env.observe()\n",
    "\t\ttimestep = 0\n",
    "\n",
    "\t\t# STARTING THE LOOP OVER ALL THE TIMESTEPS (1 Timestep = 1 Minute) IN ONE EPOCH\n",
    "\t\twhile ((not game_over) and timestep <= 5 * 30 * 24 * 60):\n",
    "\n",
    "\t\t\t# PLAYING THE NEXT ACTION BY EXPLORATION\n",
    "\t\t\tif np.random.rand() <= epsilon:\n",
    "\t\t\t\taction = np.random.randint(0, number_actions)\n",
    "\t\t\t\tif (action - direction_boundary < 0): \n",
    "\t\t\t\t\tdirection = -1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdirection = 1\n",
    "\t\t\t\tenergy_ai = abs(action - direction_boundary) * temperature_step \n",
    "\n",
    "\n",
    "\t\t\t# PLAYING THE NEXT ACTION BY INFERENCE\n",
    "\t\t\telse:\n",
    "\t\t\t\tq_values = model.predict(current_state) \n",
    "\t\t\t\taction = np.argmax(q_values[0])\n",
    "\t\t\t\tif (action - direction_boundary < 0): \n",
    "\t\t\t\t\tdirection = -1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdirection = 1\n",
    "\t\t\t\tenergy_ai = abs(action - direction_boundary) * temperature_step \n",
    "\n",
    "\n",
    "\t\t\t# UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE\n",
    "\t\t\tnext_state, reward, game_over = env.update_env(direction, energy_ai, int(timestep / (30*24*60)))\n",
    "\t\t\ttotal_reward += reward\n",
    "            \n",
    "\t\t\t# STORING THIS NEW TRANSITION INTO THE MEMORY\n",
    "\t\t\tdqn.remember([current_state, action, reward, next_state], game_over)\n",
    "\n",
    "\n",
    "\n",
    "\t\t\t# GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS\n",
    "\t\t\tinputs, targets = dqn.get_batch(model, batch_size = batch_size)\n",
    "\n",
    "\t\t\t# COMPUTING THE LOSS OVER THE TWO WHOLE BATCHES OF INPUTS AND TARGETS\n",
    "\t\t\tloss += model.train_on_batch(inputs, targets)\n",
    "\t\t\ttimestep += 1\n",
    "\t\t\tcurrent_state = next_state\n",
    "\n",
    "\n",
    "\t\t# PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
    "\t\tprint(\"\\n\")\n",
    "\t\tprint(\"Epoch: {:03d}/{:03d}\".format(epoch, number_epochs))\n",
    "\t\tprint(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
    "\t\tprint(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai)) \n",
    "\n",
    "\t\t# EARLY STOPPING\n",
    "\t\tif (early_stopping):\n",
    "\t\t\tif (total_reward <= best_total_reward):\n",
    "\t\t\t\tpatience_count += 1\n",
    "\t\t\telif (total_reward > best_total_reward):\n",
    "\t\t\t\tbest_total_reward = total_reward\n",
    "\t\t\t\tpatience_count = 0\n",
    "\t\t\n",
    "\t\tif (patience_count >= patience):\n",
    "\t\t\tprint(\"Early Stopping\")\n",
    "\t\t\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE MODEL\n",
    "\n",
    "model.save(run+\".h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "step4_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd09d382f3539cbd38332cb67532568dd124e0d28827e8f6bc80edbc6bd38de1a6c"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
